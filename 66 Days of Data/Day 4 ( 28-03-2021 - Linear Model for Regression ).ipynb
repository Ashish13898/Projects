{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model for Regression\n",
    "\n",
    "- Linear model make prediction using a linear function of the input features. \n",
    "    - y = w[0] * x[0] +...+ b --> here x is a feature and w (Slope) and b(The Offset) are the parameters of the model that are learned and y is the prediction that the model makes\n",
    "- If score is almost same as training for test then most likely it will be a under/overfit \n",
    "\n",
    "- If training score is really amazing but testing score is not then also it will be a overfit model as we need the sweet slot the middle layer\n",
    "\n",
    "NOTE : IF WE HAVE ENOUGH DATA THEN IT WILL BE HARD FOR THE MODEL TO OVERFIT AND AS A RESULT WE WILL HAVE THE SAME PERFORMANCE AS RIDGE AND LINEAR REGRESSION, RIDGE GIVES BETTER PERFORMANCE FOR SMALL DATASETS WHERE LINEAR REGRESSION COULD NOT EVEN PREDICT THE RESULTS.\n",
    "\n",
    "### Linear Regression AKA Ordinary least squares\n",
    "\n",
    "- it finds the parameters w and b that minimize the mean squared error between predictions and targets of the training set\n",
    "- the mean squared error is basically the (predicted value)^2 - (actual outut)^2 + ..  + / number of samples  \n",
    "- no parameters like KNN therefore no way to control the model complexity as well\n",
    "- We can not complete the model complexity in this\n",
    "\n",
    "### REGULARIZATION \n",
    "    - REGULARIZATION means explictly restricting your model from overfitting. However this will also end up decreasing the training set accuracy but yeah it will give more accuracy on the prediction of the test data then the linear regression\n",
    "    \n",
    "### Ridge Regression:\n",
    "\n",
    "- Ridge is basically a clone version of the linear regression but for this we are controlling the NON-ZERO coefficients (W) to predict the data as well as to fit the additional constraint(b), in a nutsheel we want our w to be small as possible as it can so that the effect of an single feature on the outcome will be small ( which is basically having an small scope ). \n",
    "    \n",
    "    \n",
    "    -Ridge use L2 Regularization\n",
    "    \n",
    "### Lasso : \n",
    "\n",
    "- It uses the same model as ridge trying to lower the values of W but the only difference is that it could 0 as well which means the model could ignore a feature as well which makes it easier for the model to focus on more important features of the model\n",
    "\n",
    "- This is called R1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Added\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6592061059587275\n",
      "0.6932519118518163\n",
      "--------------------\n",
      "0.9520519609032729\n",
      "0.607472195966585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X,y = mglearn.datasets.make_wave(n_samples = 60) # one feature\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n",
    "lr = LinearRegression().fit(X_train,y_train)\n",
    "\n",
    "print(lr.score(X_train,y_train))\n",
    "print(lr.score(X_test,y_test))\n",
    "\n",
    "print('--------------------')\n",
    "\n",
    "X,y = mglearn.datasets.load_extended_boston()\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n",
    "\n",
    "lr = LinearRegression().fit(X_train,y_train)\n",
    "\n",
    "print(lr.score(X_train,y_train))\n",
    "print(lr.score(X_test,y_test))\n",
    "\n",
    "# as we can see both values of training sets are almost same as the training which means we are underfitting as we are just using single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44153666]\n",
      "-0.01711124414733381\n"
     ]
    }
   ],
   "source": [
    "# W is basically called as coefficients as well and so as b called as intercept\n",
    "print(lr.coef_) # numpy array with length equals to number of features\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885796658517094\n",
      "0.7527683481744754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# Ridge has an parameter which is default set to 1 Ridge(alpha = 1.0), if we vary it we would see different results\n",
    "X,y = mglearn.datasets.load_extended_boston()\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n",
    "\n",
    "ridge = Ridge().fit(X_train,y_train)\n",
    "\n",
    "print(ridge.score(X_train,y_train))\n",
    "print(ridge.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29323768991114607\n",
      "0.20937503255272294\n",
      "Feature Used --> 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "# Ridge has an parameter which is default set to 1 Ridge(alpha = 1.0), if we vary it we would see different results\n",
    "X,y = mglearn.datasets.load_extended_boston()\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n",
    "\n",
    "lasso = Lasso().fit(X_train,y_train)\n",
    "\n",
    "print(lasso.score(X_train,y_train))\n",
    "print(lasso.score(X_test,y_test))\n",
    "print('Feature Used --> {}'.format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8962226511086497\n",
      "0.7656571174549983\n",
      "Feature Used --> 33\n"
     ]
    }
   ],
   "source": [
    "# Changing the alpha value, we have to increase the iterations if we decrease the alpha \n",
    "lasso = Lasso(alpha = 0.01,max_iter=100000).fit(X_train,y_train)\n",
    "\n",
    "print(lasso.score(X_train,y_train))\n",
    "print(lasso.score(X_test,y_test))\n",
    "print('Feature Used --> {}'.format(np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "# SETTING ALPHA TOO LOW COULD END UP GETTING OVERFIT MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, ridge regression is usually the first choice between these two models.\n",
    "However, if you have a large amount of features and expect only a few of them to be\n",
    "important,\n",
    "might be a better choice\n",
    "Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
